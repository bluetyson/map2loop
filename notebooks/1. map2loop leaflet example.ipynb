{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## map2loop: Western Australia\n",
    "\n",
    "This notebook allows you to select a rectangular sub-area in Western Asutralia, and using <a href=\"https://www.dmp.wa.gov.au/Geological-Survey/Geological-Survey-262.aspx\">GSWA</a>  geology polygons, bedding orientation data and fault polylines, calculates the topological relationships between the different features. All the data is Open Access and is served on a Loop WFS server.\n",
    "\n",
    "This all gets fed into successive tolopogical and geometric transfroms that end up feeding into a mdelling engine to make a 3D model.  \n",
    "  \n",
    "\n",
    "<font color='red'>The leaflet version here dangerously allows you to select your own area from the map, this is just for showing off, as in many parts of Western Australia, the area we focus on here, there isn't enough data in the map alone to make a decent 3D model</font>   \n",
    "\n",
    "The outputs from this notebook can be fed into gempy, LoopStructural, Geomodeller &, for the faults, noddy. Having some data doen't mean that you will get a reasonable model for three reasons:\n",
    "   \n",
    "1) The map is itself a model, and it may be wrong, or internally inconsistent  \n",
    "\n",
    "2) map2loop may have stuffed up   \n",
    "\n",
    "3) The modelling algorithm may be too simple to accept all the data, or to build a geologically reasonable model with even good data.   \n",
    "\n",
    "Codes here were developed as part of the <a href=\"https://loop3d.org/\">Loop</a> / <a href=\"https://minexcrc.com.au/\">MinEx CRC</a> / <a href =\"https://www.darecentre.org.au/\">DARE</a> partnerships with export codes drawing heavily on the expertise of Miguel de la Varga (<a href=\"https://github.com/cgre-aachen/gempy\">gempy</a>), Lachlan Grose (Loop Structural, coming soon!), Des FItzgerald (<a href=\"https://www.intrepid-geophysics.com/product/geomodeller/Geomodeller\">Geomodeller</a>) and Florian Wellmann & Noe Pollack (<a href=\"https://github.com/cgre-aachen/pynoddy\">pynoddy</a>).\n",
    "\n",
    "### Instructions:   \n",
    "1) Select rectangular area from map below, in an area with some bedding data (red dots). *I advise you start off near where the map open ups below, the further you stray, the harder it gets!*  \n",
    "   \n",
    "2) Select a target modelling engine (loopstructural, gempy, geomodeller, noddy)   \n",
    "\n",
    "3) Run all code below that and post an issue at <a href=\"https://github.com/Loop3D/map2loop/issues\">github</a> when it crashes   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "from shapely.geometry import Polygon\n",
    "from ipyleaflet import Map, basemaps, GeoJSON, LayersControl, DrawControl,WMSLayer\n",
    "from ipywidgets import Label\n",
    "import ipywidgets as widgets\n",
    "import geopandas as gpd\n",
    "\n",
    "wms_warox = WMSLayer(\n",
    "    url='http://geo.loop-gis.org/geoserver/loop/wms?',\n",
    "    layers='loop:waroxi_wa_4326_bed',\n",
    "    format='image/png',\n",
    "    transparent=True,\n",
    "    attribution='Outcrop data from GSWA',\n",
    "    name='outcrops'\n",
    "\n",
    ")\n",
    "wms_geol = WMSLayer(\n",
    "    url='http://geo.loop-gis.org/geoserver/loop/wms?',\n",
    "    layers='loop:2_5m_interpgeop15_4326',\n",
    "    format='image/png',\n",
    "    transparent=True,\n",
    "    opacity=0.4,\n",
    "    attribution='Geology data from GSWA',\n",
    "    name='geology'\n",
    "\n",
    ")\n",
    "m =Map(basemap=basemaps.OpenTopoMap, center=(-22.6,117.3), zoom=9,scroll_wheel_zoom=True)\n",
    "\n",
    "m.add_layer(wms_geol)\n",
    "m.add_layer(wms_warox)\n",
    "\n",
    "m.add_control(LayersControl())\n",
    "dc = DrawControl(rectangle={'shapeOptions': {'color': '#0000FF'}})\n",
    "m.add_control(dc)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:02.266775Z",
     "start_time": "2020-06-02T06:36:02.233778Z"
    }
   },
   "outputs": [],
   "source": [
    "new_poly=GeoJSON(data=dc.last_draw)\n",
    "new_poly=str(new_poly)\n",
    "\n",
    "if(\"'geometry': None\" in new_poly):\n",
    "    raise NameError('map2loop error: No rectangle selected')\n",
    "new_poly=new_poly.rsplit(\"'coordinates': \", 1)[1]\n",
    "new_poly=new_poly.replace('[[[','').replace('[','').replace(']]]}})','').replace('],','').replace(',','').split(\" \")\n",
    "longs=new_poly[0::2]\n",
    "lats=new_poly[1::2]\n",
    "minlong=float(min(longs))\n",
    "maxlong=float(max(longs))\n",
    "minlat=float(max(lats)) #ignores sign\n",
    "maxlat=float(min(lats)) #ignores sign\n",
    "bounds=(minlong,maxlong,minlat,maxlat)\n",
    "src_crs = \"epsg:4326\"  # coordinate reference system for imported dtms (geodetic lat/long WGS84)\n",
    "dst_crs = \"epsg:28350\" # coordinate system for example data\n",
    "\n",
    "\n",
    "lat_point_list = [minlat, minlat, maxlat, maxlat,maxlat]\n",
    "lon_point_list = [minlong, maxlong, maxlong, minlong, minlong]\n",
    "bbox_geom = Polygon(zip(lon_point_list, lat_point_list))\n",
    "mbbox = gpd.GeoDataFrame(index=[0], crs=src_crs, geometry=[bbox_geom]) \n",
    "display(src_crs,mbbox.total_bounds)\n",
    "mbbox=mbbox.to_crs(dst_crs)\n",
    "display(dst_crs,mbbox.total_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose model engine\n",
    "1. Run cell below\n",
    "2. Select modelling engine from drop down menu   \n",
    "3. Click in next cell and run to end using menu: *Cell->Run all below* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:02.295774Z",
     "start_time": "2020-06-02T06:36:02.272778Z"
    }
   },
   "outputs": [],
   "source": [
    "engine_choice=widgets.Dropdown(\n",
    "    options=['geomodeller', 'gempy','loopstructural','noddy','null'],\n",
    "    value='loopstructural',\n",
    "    description='Modeller:',\n",
    "    disabled=False,\n",
    ")\n",
    "display(engine_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:09.098804Z",
     "start_time": "2020-06-02T06:36:02.324778Z"
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import stat\n",
    "import functools \n",
    "import operator  \n",
    "import matplotlib\n",
    "import networkx as nx\n",
    "import rasterio\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Polygon\n",
    "from map2loop import m2l_utils\n",
    "from map2loop import m2l_topology\n",
    "from map2loop import m2l_geometry\n",
    "from map2loop import m2l_interpolation\n",
    "from map2loop import m2l_export\n",
    "from map2loop import m2l_map_checker\n",
    "import time\n",
    "import shutil\n",
    "%matplotlib inline\n",
    "t0 = time.time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:09.133807Z",
     "start_time": "2020-06-02T06:36:09.103808Z"
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data_path='../scratch/'\n",
    "minx=mbbox.total_bounds[0]\n",
    "maxx=mbbox.total_bounds[2]\n",
    "miny=mbbox.total_bounds[1]\n",
    "maxy=mbbox.total_bounds[3]\n",
    "bbox2=str(minx)+\",\"+str(miny)+\",\"+str(maxx)+\",\"+str(maxy)\n",
    "os.chdir('../source_data/')\n",
    "%run -i \"m2l_config_remote.py\"\n",
    "#%run -i \"m2l_config_remote.py\"\n",
    "print(os.getcwd())\n",
    "\n",
    "\n",
    "\n",
    "lat_point_list = [miny, miny, maxy, maxy, maxy]\n",
    "lon_point_list = [minx, maxx, maxx, minx, minx]\n",
    "bbox_geom = Polygon(zip(lon_point_list, lat_point_list))\n",
    "polygon = gpd.GeoDataFrame(index=[0], crs=dst_crs, geometry=[bbox_geom]) \n",
    "bbox=(minx,miny,maxx,maxy)\n",
    "\n",
    "dc.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:09.160806Z",
     "start_time": "2020-06-02T06:36:09.138805Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# opportunity to second guess config file or add extra parameters not yet in config file...\n",
    "\n",
    "fold_decimate=2         \n",
    "\n",
    "fault_decimate=0\n",
    "\n",
    "contact_decimate=5\n",
    "\n",
    "orientation_decimate=0\n",
    "\n",
    "use_interpolations=True       #use interpolated dips/contacts as additional constraints\n",
    "\n",
    "use_fat=True                   #use fold axial trace orientation hints\n",
    "\n",
    "pluton_form='domes'\n",
    "\n",
    "fault_dip=90\n",
    "\n",
    "min_fault_length=5000\n",
    "\n",
    "compute_etc=False\n",
    "\n",
    "#spacing > 0 gives absolute spacing, spacing <0 gives # of grid points in x direction\n",
    "spacing=-200\n",
    "\n",
    "#spacing=500   #grid spacing in metres of interpolation points\n",
    "\n",
    "\n",
    "local_paths=False\n",
    "\n",
    "#################################\n",
    "# There are many alternative datasets that \n",
    "# can be extracted from the input data, \n",
    "# and many choices of possible input data\n",
    "#\n",
    "# These flags define what the actual workflow \n",
    "# will be for this experiment, based partly \n",
    "# on which (if any) modelling engine is used\n",
    "#\n",
    "#############################################\n",
    "workflow={'model_engine':engine_choice.value} \n",
    "\n",
    "if(workflow['model_engine']=='geomodeller'):\n",
    "    workflow.update( {'seismic_section':False,\n",
    "          'cover_map':False,\n",
    "          'near_fault_interpolations':True,\n",
    "          'fold_axial_traces':False,\n",
    "          'stereonets':True,\n",
    "          'formation_thickness':True,\n",
    "          'polarity':False,\n",
    "          'strat_offset':True,\n",
    "          'contact_dips':True} )\n",
    "elif(workflow['model_engine']=='loopstructural'):\n",
    "    workflow.update( {'seismic_section':False,\n",
    "          'cover_map':False,\n",
    "          'near_fault_interpolations':False,\n",
    "          'fold_axial_traces':True,\n",
    "          'stereonets':True,\n",
    "          'formation_thickness':True,\n",
    "          'polarity':False,\n",
    "          'strat_offset':False,\n",
    "          'contact_dips':True} )\n",
    "elif(workflow['model_engine']=='gempy'):\n",
    "    workflow.update( {'seismic_section':False,\n",
    "          'cover_map':False,\n",
    "          'near_fault_interpolations':False,\n",
    "          'fold_axial_traces':True,\n",
    "          'stereonets':False,\n",
    "          'formation_thickness':False,\n",
    "          'polarity':False,\n",
    "          'strat_offset':False,\n",
    "          'contact_dips':False} )\n",
    "elif(workflow['model_engine']=='noddy'):\n",
    "    workflow.update( {'seismic_section':False,\n",
    "          'cover_map':False,\n",
    "          'near_fault_interpolations':False,\n",
    "          'fold_axial_traces':False,\n",
    "          'stereonets':False,\n",
    "          'formation_thickness':False,\n",
    "          'polarity':False,\n",
    "          'strat_offset':False,\n",
    "          'contact_dips':False} )\n",
    "else:\n",
    "    workflow.update( {'seismic_section':False,\n",
    "          'cover_map':False,\n",
    "          'near_fault_interpolations':False,\n",
    "          'fold_axial_traces':False,\n",
    "          'stereonets':True,\n",
    "          'formation_thickness':True,\n",
    "          'polarity':False,\n",
    "          'strat_offset':True,\n",
    "          'contact_dips':False} )\n",
    "    \n",
    "\n",
    "\n",
    "# no cover info so no need load cover layers\n",
    "if(not workflow['cover_map']):\n",
    "    dtb=0\n",
    "    dtb_null=0\n",
    "else:\n",
    "    dtb_grid=data_path+'young_cover_grid.tif' #obviously hard-wired for the moment\n",
    "    dtb_null='-2147483648' #obviously hard-wired for the moment\n",
    "    cover_map_path=data_path+'Young_Cover_FDS_MGA_clean.shp' #obviously hard-wired for the moment\n",
    "    dtb_clip=output_path+'young_cover_grid_clip.tif' #obviously hard-wired for the moment\n",
    "    cover_dip=10 # dip of cover away from contact\n",
    "    spacing=5000 # of contact grid in metres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we test to see if we have access to the online data we need\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:09.528875Z",
     "start_time": "2020-06-02T06:36:09.168807Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "loopwfs=m2l_utils.have_access(\"geo.loop-gis.org\")\n",
    "ga=m2l_utils.have_access(\"services.ga.gov.au\")\n",
    "\n",
    "if(not local_paths and not loopwfs):\n",
    "    raise NameError('map2loop error: No access to remote map server')\n",
    "\n",
    "if(not (loopwfs & ga)):\n",
    "    local_paths=True\n",
    "    net=False\n",
    "    print('using local paths')\n",
    "else:\n",
    "    net=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Check Map for valid input files   \n",
    "   \n",
    "### Checks for:   \n",
    "- Files exist\n",
    "- Requred fields\n",
    "- No NaN/blanks in required fields\n",
    "- Sufficient orientation data\n",
    "- LineString/PolyLines for faults (i.e. not MultiLineStrings/MultiPolylines), if found splits into unique ID polylines\n",
    "- Commas in unit code (maybe should check in groups and alt groups??\n",
    "\n",
    "### Should also check for:   \n",
    "- Significantly overlapping polygons\n",
    "- Faults that should be joined (c.f. FracG code)\n",
    "- Orientations near contacts that make no sense (i.e. strike is at high angle to contact tangent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:11.328986Z",
     "start_time": "2020-06-02T06:36:09.533874Z"
    }
   },
   "outputs": [],
   "source": [
    "m2l_map_checker.check_map(structure_file,geology_file,fault_file,mindep_file,tmp_path,bbox,c_l,dst_crs,local_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display stereonets of bedding by formations and group to see how we can combine them later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:23.738017Z",
     "start_time": "2020-06-02T06:36:11.334988Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "geology = gpd.read_file(geology_file,bbox=bbox)\n",
    "\n",
    "geology[c_l['g']].fillna(geology[c_l['g2']], inplace=True)\n",
    "geology[c_l['g']].fillna(geology[c_l['c']], inplace=True)\n",
    "\n",
    "orientations = gpd.read_file(structure_file,bbox=bbox)\n",
    "if(len(orientations)<2):\n",
    "    raise NameError('Not enough orientations to complete calculations (need at least 2)')\n",
    "group_girdle=m2l_utils.plot_bedding_stereonets(orientations,geology,c_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test code to automatically create super_groups and use_group3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import acos, degrees\n",
    "\n",
    "group_girdle =pd.DataFrame.from_dict(group_girdle,orient='index')\n",
    "group_girdle.columns = ['plunge', 'bearing', 'num orientations']\n",
    "group_girdle.sort_values(by='num orientations', ascending=False,inplace=True)\n",
    "display(group_girdle)\n",
    "\n",
    "\n",
    "l,m,n=m2l_utils.ddd2dircos(group_girdle.iloc[0]['plunge'],group_girdle.iloc[0]['bearing'])\n",
    "super_group=pd.DataFrame([[group_girdle[0:1].index[0],'Super_Group_0',l,m,n]],columns=['Group','Super_Group','l','m','n'])\n",
    "super_group.set_index('Group',inplace=True)\n",
    "\n",
    "print('---------------------------------')\n",
    "sg_index=0\n",
    "for i in range(1,len(group_girdle)):\n",
    "    l,m,n=m2l_utils.ddd2dircos(group_girdle.iloc[i]['plunge'],group_girdle.iloc[i]['bearing'])\n",
    "    found=False\n",
    "    sg_i=0\n",
    "    for ind,sg in super_group.iterrows():\n",
    "        \n",
    "        c = sg['l']*l + sg['m']*m + sg['n']*n\n",
    "        if c>1:\n",
    "            c=1\n",
    "        c=degrees(acos(c))  \n",
    "\n",
    "        if(c<30  and not found):\n",
    "            found=True\n",
    "            sgname='Super_Group_'+str(sg_i)\n",
    "            super_group_old=pd.DataFrame([[group_girdle[i:i+1].index[0],sgname,l,m,n]],columns=['Group','Super_Group','l','m','n'])\n",
    "            super_group_old.set_index('Group',inplace=True)\n",
    "            super_group=super_group.append(super_group_old)\n",
    "        sg_i=sg_i+1\n",
    "        \n",
    "    if(not found):\n",
    "\n",
    "        sg_index=sg_index+1\n",
    "        #print('not found',sg_index)\n",
    "        sgname='Super_Group_'+str(sg_index)\n",
    "        super_group_new=pd.DataFrame([[group_girdle[i:i+1].index[0],sgname,l,m,n]],columns=['Group','Super_Group','l','m','n'])\n",
    "        super_group_new.set_index('Group',inplace=True)\n",
    "        super_group=super_group.append(super_group_new)\n",
    "\n",
    "display(super_group)  \n",
    "\n",
    "\n",
    "use_gcode3=[]\n",
    "for ind,sg in super_group.iterrows():\n",
    "    clean=ind.replace(\" \",\"_\").replace(\"-\",\"_\")\n",
    "    use_gcode3.append(clean)\n",
    "\n",
    "display(use_gcode3)\n",
    "\n",
    "sg2=set(super_group['Super_Group'])\n",
    "display(sg2)\n",
    "super_groups=[]\n",
    "for s in sg2:\n",
    "    temp=[]\n",
    "    for  ind,sg in super_group.iterrows():\n",
    "        if(s == sg['Super_Group']):\n",
    "            temp.append(ind)\n",
    "    super_groups.append(temp)\n",
    "\n",
    "display(super_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decide on super groups and groups\n",
    "Supergroups define what shall be interpolated as single system   \n",
    "use_gcode defines which groups we choose to calculate model from  \n",
    "\n",
    "<font color='red'>The following outputs are the default settings that put all groups as one super_group for orientation interpolation purposes and use_gcode3 defines which groups will actually be modelled If after looking at the stereonets you want to change these defaults, just copy paste the two following lines into the next cell below and edit them as you wish. </font>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:23.757015Z",
     "start_time": "2020-06-02T06:36:23.743015Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"super_groups=\",super_groups)\n",
    "print(\"use_gcode3=\",use_gcode3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:23.757015Z",
     "start_time": "2020-06-02T06:36:23.743015Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data to ensure it meets modelling requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('only processing',use_gcode3)\n",
    "\n",
    "inputs=('')\n",
    "\n",
    "if(workflow['model_engine'] =='geomodeller'):\n",
    "    inputs=('invented_orientations','intrusive_orientations','fat_orientations','near_fault_orientations','fault_tip_contacts','contact_orientations')\n",
    "elif(workflow['model_engine']=='loopstructural'):\n",
    "    inputs=('invented_orientations','fat_orientations','contact_orientations')\n",
    "elif(workflow['model_engine']=='gempy'):\n",
    "    inputs=('invented_orientations','interpolated_orientations','fat_orientations')\n",
    "elif(workflow['model_engine']=='noddy'):\n",
    "    inputs=('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot geology polygons and bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:24.855075Z",
     "start_time": "2020-06-02T06:36:23.763015Z"
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(geology_file)\n",
    "geology_ll = gpd.read_file(geology_file,bbox=bbox)\n",
    "\n",
    "geology_ll[c_l['g']].fillna(geology_ll[c_l['g2']], inplace=True)\n",
    "geology_ll[c_l['g']].fillna(geology_ll[c_l['c']], inplace=True)\n",
    "display(geology_ll.head())\n",
    "base=geology_ll.plot(column=c_l['c'],figsize=(10,10),edgecolor='#000000',linewidth=0.2)\n",
    "polygon.plot(ax=base, color='none',edgecolor='black')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save geology to file as WKT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:25.302073Z",
     "start_time": "2020-06-02T06:36:24.861074Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "hint_flag=False # use GSWA strat database to provide relative age hints\n",
    "sub_geol = geology_ll[['geometry', c_l['o'],c_l['c'],c_l['g'],c_l['u'],c_l['min'],c_l['max'],c_l['ds'],c_l['r1'],c_l['r2']]]\n",
    "m2l_topology.save_geol_wkt(sub_geol,geology_file_csv, c_l,hint_flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save mineral deposits to file as WKT\n",
    "This is not needed by map2loop to build 3D models, but is used by map2model to calculate mineral deposit/topology analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:26.206626Z",
     "start_time": "2020-06-02T06:36:25.308075Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "mindep = gpd.read_file(mindep_file,bbox=bbox)\n",
    "\n",
    "sub_mindep = mindep[['geometry', c_l['msc'],c_l['msn'],c_l['mst'],c_l['mtc'],c_l['mscm'],c_l['mcom']]]\n",
    "m2l_topology.save_mindep_wkt(sub_mindep,mindep_file_csv, c_l)\n",
    "\n",
    "base=sub_mindep.plot()\n",
    "polygon.plot(ax=base, color='none',edgecolor='black')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and save orientations data point data as WKT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:26.990149Z",
     "start_time": "2020-06-02T06:36:26.213605Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "orientations = gpd.read_file(structure_file,bbox=bbox)\n",
    "\n",
    "sub_pts = orientations[['geometry', c_l['gi'],c_l['d'],c_l['dd']]]\n",
    "\n",
    "m2l_topology.save_structure_wkt(sub_pts,structure_file_csv,c_l)\n",
    "\n",
    "base=sub_pts.plot()\n",
    "polygon.plot(ax=base, color='none',edgecolor='black')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot faults and bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:27.704149Z",
     "start_time": "2020-06-02T06:36:26.996145Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "lines_ll=gpd.read_file(fault_file,bbox=bbox)\n",
    "\n",
    "base2=lines_ll.plot(cmap='rainbow',column=c_l['f'],figsize=(10,10),linewidth=0.4)\n",
    "polygon.plot(ax=base2, color='none',edgecolor='black')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save faults to file as WKT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:27.829145Z",
     "start_time": "2020-06-02T06:36:27.711143Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "sub_lines = lines_ll[['geometry', c_l['o'],c_l['f']]]\n",
    "m2l_topology.save_faults_wkt(sub_lines,fault_file_csv,c_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create map2model input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:27.851146Z",
     "start_time": "2020-06-02T06:36:27.835146Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "m2l_topology.save_Parfile(m2m_cpp_path,c_l,graph_path,geology_file_csv,fault_file_csv,structure_file_csv,mindep_file_csv,minx,maxx,miny,maxy,500.0,'Fe,Cu,Au,NONE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:31.879760Z",
     "start_time": "2020-06-02T06:36:27.859150Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import platform\n",
    "os.chdir(m2m_cpp_path)\n",
    "print(os.getcwd())\n",
    "#%system map2model.exe Parfile\n",
    "if(platform.system()=='Windows'):\n",
    "    subprocess.run([\"map2model.exe\", \"Parfile\"])\n",
    "else:\n",
    "    subprocess.run([\"./map2model\", \"Parfile\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple network graph of the geology with legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:33.117911Z",
     "start_time": "2020-06-02T06:36:31.884772Z"
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "G=nx.read_gml(strat_graph_file,label='id')\n",
    "selected_nodes = [n for n,v in G.nodes(data=True) if n >=0]\n",
    "nx.draw_networkx(G, pos=nx.kamada_kawai_layout(G), arrows=True, nodelist=selected_nodes)\n",
    "\n",
    "nlist=list(G.nodes.data('LabelGraphics'))\n",
    "nlist.sort()\n",
    "for no in nlist:\n",
    "    if(no[0]>=0):\n",
    "        elem=str(no[1]).replace(\"{'text':\",\"\").replace(\", 'fontSize': 14}\",\"\")\n",
    "        #second=elem.split(\":\").replace(\"'\",\"\")\n",
    "        print(no[0],\" \",elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Process topography, stratigraphy, fold axial traces and faults\n",
    "\n",
    "### Takes GML file produced by topology code, combines with geology polygons, structure points and dtm to create 3D model in gempy.<br><br>\n",
    "\n",
    "Limitations:  no dykes, no sills. Sills require us to assign a unique surface to each instance of a sill (sill between units A and B needs to be different from sill of same age and strat codes as one found between E and F). Dykes via cokriging are really hard without just cookie cutting them in (but that is not our problem!). We are not checking for onlap relationships, which can perhaps been seen by having lots of units from one series adjacent to the youngest surface of the older series. Could also think about interpreting these as faults to introduce conceptual uncertainty. All mistakes belong to Mark Jessell, topology code that feeds this system by Vitaliy Ogarko.<br><br>\n",
    "\n",
    "Geology layer needs to have some unique strat code or text, some group code or text to function<br>\n",
    "Structure layer needs dip/dip direction<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:33.135909Z",
     "start_time": "2020-06-02T06:36:33.123906Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "os.chdir('../map2loop')\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "#from IPython.core.display import display, HTML\n",
    "#display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "\n",
    "sys.path.insert(0,\"../..\")\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next we define an area of interest and some other basic stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:33.159908Z",
     "start_time": "2020-06-02T06:36:33.141909Z"
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "\n",
    "bbox2=str(minx)+\",\"+str(miny)+\",\"+str(maxx)+\",\"+str(maxy)\n",
    "lat_point_list = [miny, miny, maxy, maxy, maxy]\n",
    "lon_point_list = [minx, maxx, maxx, minx, minx]\n",
    "bbox_geom = Polygon(zip(lon_point_list, lat_point_list))\n",
    "polygon = gpd.GeoDataFrame(index=[0], crs=dst_crs, geometry=[bbox_geom]) \n",
    "bbox=(minx,miny,maxx,maxy)\n",
    "\n",
    "step_out=0.045 #add (in degrees) so edge pixel from dtm reprojection are not found\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Download and reproject the appropriate SRTM data\n",
    "mj: Getting this from GA, but could also get from Hawaii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:35.360910Z",
     "start_time": "2020-06-02T06:36:33.170933Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "polygon_ll=polygon.to_crs(src_crs)\n",
    "\n",
    "minlong=polygon_ll.total_bounds[0]-step_out\n",
    "maxlong=polygon_ll.total_bounds[2]+step_out\n",
    "minlat=polygon_ll.total_bounds[1]-step_out\n",
    "maxlat=polygon_ll.total_bounds[3]+step_out\n",
    "\n",
    "print(minlong,maxlong,minlat,maxlat)\n",
    "#if(((not os.path.exists(dtm_file)) or (not local_paths)) and net):    \n",
    "m2l_utils.get_dtm(dtm_file, minlong,maxlong,minlat,maxlat)\n",
    "geom_rp=m2l_utils.reproject_dtm(dtm_file,dtm_reproj_file,src_crs,dst_crs)\n",
    "\n",
    "dtm = rasterio.open(dtm_reproj_file)\n",
    "pyplot.imshow(dtm.read(1), cmap='terrain',vmin=0,vmax=1000)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Load stratigraphy graph and create list of series (aka groups)\n",
    "mj: The choice of what constitutes basic unit and what a group of units is hard-wired at the moment, but could be altered to any pair. Not even sure we need two levels but it seemed like a good idea at the time. Note that this needs the arcgis plugin version of the topology code (for now) as it seperates the different sub graphs. Text outputs list alternate topologies for series and surfaces, which if confirmed by comapring max-min ages will be a nice source of uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:37.574913Z",
     "start_time": "2020-06-02T06:36:35.371908Z"
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "groups,glabels,G = m2l_topology.get_series(strat_graph_file,'id')\n",
    "m2l_topology.save_units(G,tmp_path,glabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Load geology & structure data\n",
    "Currently loading from local files, but could load geology from WFS server at GSWA EXCEPT that the WFS online map has less fields that the zipped shapefiles. Go figure. We don't use fault layer at the moment (except for Vitaliy's topology code) but same logic applies in terms of where to get it from. Already have fault/strat relationships and once we have fault/fault relationships will start to include faults in models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:38.727909Z",
     "start_time": "2020-06-02T06:36:37.580908Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Extract point data from structure & geology layers for modelling\n",
    "##First we readin the structure and map from shapefiles, or wherever...\n",
    "\n",
    "bbox=(minx,miny,maxx,maxy)\n",
    "geology = gpd.read_file(geology_file,bbox=bbox)\n",
    "geology[c_l['g']].fillna(geology[c_l['g2']], inplace=True)\n",
    "geology[c_l['g']].fillna(geology[c_l['c']], inplace=True)\n",
    "\n",
    "\n",
    "structure = gpd.read_file(structure_file,bbox=bbox)\n",
    "structure.crs=dst_crs\n",
    "print(fault_file)\n",
    "faults = gpd.read_file(fault_file,bbox=bbox)\n",
    "faults.crs=dst_crs\n",
    "\n",
    "sub_pts = structure[['geometry',c_l['d'],c_l['dd'],c_l['sf']]] \n",
    "\n",
    "base=geology.plot(column=c_l['c'],figsize=(10,10),edgecolor='#000000',linewidth=0.2)\n",
    "sub_pts.plot(ax=base,edgecolor='black')\n",
    "faults.plot(ax=base, column=c_l['f'],edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Clip geology, faults, structures and map geology to structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:40.957911Z",
     "start_time": "2020-06-02T06:36:38.733913Z"
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "geology = m2l_utils.explode(geology)\n",
    "geology.crs = dst_crs\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows',None)\n",
    "structure_code = gpd.sjoin(sub_pts, geology, how=\"left\", op=\"within\")\n",
    "\n",
    "y_point_list = [miny, miny, maxy, maxy, miny]\n",
    "x_point_list = [minx, maxx, maxx, minx, minx]\n",
    "\n",
    "bbox_geom = Polygon(zip(x_point_list, y_point_list))\n",
    "\n",
    "polygo = gpd.GeoDataFrame(index=[0], crs=dst_crs, geometry=[bbox_geom]) \n",
    "#display(polygo.geometry)\n",
    "is_bed=structure_code[c_l['sf']].str.contains(c_l['bedding'], regex=False) \n",
    "    \n",
    "all_beds = structure_code[is_bed]\n",
    "\n",
    "if(workflow['fold_axial_traces']):\n",
    "    all_folds=faults[faults[c_l['f']].str.contains(c_l['fold'])]\n",
    "    folds_clip=m2l_utils.clip_shp(all_folds,polygo)\n",
    "    folds_clip.crs = dst_crs\n",
    "\n",
    "    folds_clip.to_file(tmp_path+'folds_clip.shp')\n",
    "\n",
    "\n",
    "all_faults=faults[faults[c_l['f']].str.contains(c_l['fault'])]\n",
    "\n",
    "\n",
    "geol_clip=gpd.overlay(geology, polygo, how='intersection')\n",
    "faults_clip=m2l_utils.clip_shp(all_faults,polygo)\n",
    "\n",
    "structure_clip = m2l_utils.clip_shp(all_beds, polygo)\n",
    "\n",
    "base = geol_clip.plot(column=c_l['c'],figsize=(7,7),edgecolor='#000000',linewidth=0.2)\n",
    "faults_clip.plot(ax=base, column=c_l['f'],edgecolor='black')\n",
    "structure_clip.plot(ax=base, column=c_l['c'],edgecolor='black')\n",
    "\n",
    "\n",
    "#if(c_l['dd']=='strike'):\n",
    "#    structure_clip['azimuth2'] = structure_clip.apply(lambda row: row[c_l['dd']]+90.0, axis = 1)\n",
    "#    c_l['dd']='azimuth2'\n",
    "    \n",
    "geol_clip.to_file(tmp_path+'geol_clip.shp')\n",
    "structure_clip.crs = dst_crs\n",
    "structure_clip.to_file(tmp_path+'structure_clip.shp')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create possible stratigraphy sets per group\n",
    "mj: <font color='red'>Uses first of each possible set of toplogies per unit and per group, which is arbitrary. </font>On the other hand we are not checking relative ages again to see if this helps reduce ambiguity, which I think it would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:41.820914Z",
     "start_time": "2020-06-02T06:36:40.962912Z"
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m2l_topology.save_group(G,tmp_path,glabels,geol_clip,c_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate cover depth grid and contacts    \n",
    "Grid is assumed to be of depth below surface   \n",
    "\n",
    "Also need to cater for case when we only have grid, no shapefile, so need to add a fake horizontal orientation in the middle of the map at average depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:41.845914Z",
     "start_time": "2020-06-02T06:36:41.826915Z"
    }
   },
   "outputs": [],
   "source": [
    "if(workflow['cover_map']):\n",
    "\n",
    "    dtm = rasterio.open(dtm_reproj_file)\n",
    "    dtb_raw = rasterio.open(dtb_grid)\n",
    "\n",
    "    cover=gpd.read_file(cover_map_path)\n",
    "\n",
    "    with fiona.open(cover_map_path, \"r\") as shapefile:\n",
    "        shapes = [feature[\"geometry\"] for feature in shapefile]\n",
    "\n",
    "    with rasterio.open(dtb_grid) as src:\n",
    "        out_image, out_transform = rasterio.mask.mask(src, shapes, crop=True)\n",
    "        out_meta = src.meta.copy()\n",
    "\n",
    "    out_meta.update({\"driver\": \"GTiff\",\n",
    "                     \"height\": out_image.shape[1],\n",
    "                     \"width\": out_image.shape[2],\n",
    "                     \"transform\": out_transform})\n",
    "\n",
    "    with rasterio.open(dtb_clip, \"w\", **out_meta) as dest:\n",
    "        dest.write(out_image)  \n",
    "\n",
    "    dtb = rasterio.open(dtb_clip)\n",
    "    \n",
    "    m2l_geometry.process_cover(output_path,dtm,dtb,dtb_null,cover,workflow['cover_map'],cover_dip,bbox,dst_crs,spacing,contact_decimate=3,use_vector=True,use_grid=True)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Export orientation data in csv  format\n",
    "mj: Orientation data needs calculated height as file does not provide it, taken from SRTM data already downloaded. To calculate polarity <font color='red'>(WHICH WE DON'T DO YET)</font> we can calculate the dot product of the dip direction of a bedding plane and the vector to that points nearest basal contact node, if  abs(acos(dot product))>90  then right way up?\n",
    "\n",
    "\n",
    "\n",
    "Added code to not save intrusion orientation data as they won't have associated surfaces if sill..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:42.781913Z",
     "start_time": "2020-06-02T06:36:41.851916Z"
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m2l_geometry.save_orientations(structure_clip,output_path,c_l,orientation_decimate,dtm,dtb,dtb_null,workflow['cover_map'])\n",
    "\n",
    "m2l_utils.plot_points(output_path+'orientations.csv',geol_clip, 'formation','X','Y',True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Find those series that don't have any orientation or contact point data  then create arbitrary point for series with no orientation data\n",
    "Not sure if gempy needs this but geomodeller does. Currently just gives a point dipping 45 degrees to North, but could use dip direction normal to basal surface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:43.885916Z",
     "start_time": "2020-06-02T06:36:43.632913Z"
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m2l_geometry.create_orientations( tmp_path, output_path, dtm,dtb,dtb_null,workflow['cover_map'],geol_clip,structure_clip,c_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Export contact information subset of each polygon to gempy format\n",
    "mj: Orientation data needs calculated height as file does not provide it, taken from SRTM data already downloaded. Need to reduce number of points whilst retaining useful info (Ranee's job!)'\n",
    "To calculate which are the basal units contact for a polygon find the polygons which are older than the selected polygon, in the example below the central polygon has relative age 23 so its basal contact is with the polygons whose ages are 26 & 28. If there are no older units for a polygon it has no basal content. We keep every nth node based on the decimate term (simple count along polyline). gempy seems to need at least two points per surface, so we always take the first two points.\n",
    "\n",
    "\n",
    "<img src='../graphics/base.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:47.910041Z",
     "start_time": "2020-06-02T06:36:43.890916Z"
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls_dict,ls_dict_decimate=m2l_geometry.save_basal_contacts(tmp_path,dtm,dtb,dtb_null,workflow['cover_map'],geol_clip,contact_decimate,c_l,intrusion_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove all basal contacts that are defined by faults and save to shapefile (no decimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:48.679043Z",
     "start_time": "2020-06-02T06:36:47.916041Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "m2l_geometry.save_basal_no_faults(tmp_path+'basal_contacts.shp',tmp_path+'faults_clip.shp',ls_dict,10,c_l,dst_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove faults from decimated basal contacts as save as csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:51.131041Z",
     "start_time": "2020-06-02T06:36:48.687041Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "contacts=gpd.read_file(tmp_path+'basal_contacts.shp')\n",
    "\n",
    "m2l_geometry.save_basal_contacts_csv(contacts,output_path,dtm,dtb,dtb_null,workflow['cover_map'],contact_decimate,c_l)\n",
    "\n",
    "m2l_utils.plot_points(output_path+'contacts4.csv',geol_clip, 'formation','X','Y',False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New interpolation test\n",
    "Interpolates a regular grid of orientations from an  shapefile of arbitrarily-located points and saves out four csv files of l,m & n direction cosines and dip dip direction data\n",
    "\n",
    "Can choose between various RBF and IDW options   \n",
    "  \n",
    "The purpose of these interpolations and associated code is to help in three cases:\n",
    "- Providing estimated dips and contacts in fault-bounded domains where no structural data are available\n",
    "- Needed to estimate true thickness of formations\n",
    "- Possibly useful for populating parts of maps where little structural data is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:56.182700Z",
     "start_time": "2020-06-02T06:36:51.139045Z"
    }
   },
   "outputs": [],
   "source": [
    "basal_contacts=tmp_path+'basal_contacts.shp'\n",
    "\n",
    "\n",
    "\n",
    "orientation_interp,contact_interp,combo_interp=m2l_interpolation.interpolation_grids(geology_file,structure_file,basal_contacts,bbox,spacing,dst_crs,scheme,super_groups,c_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:56.182700Z",
     "start_time": "2020-06-02T06:36:51.139045Z"
    }
   },
   "outputs": [],
   "source": [
    "f=open(tmp_path+'interpolated_orientations.csv','w')\n",
    "f.write('X,Y,l,m,n,dip,dip_dir\\n')\n",
    "for row in orientation_interp:\n",
    "    ostr='{},{},{},{},{},{},{}\\n'.format(row[0],row[1],row[2],row[3],row[4],row[5],row[6])\n",
    "    f.write(ostr)\n",
    "f.close()\n",
    "f=open(tmp_path+'interpolated_contacts.csv','w')\n",
    "f.write('X,Y,l,m,angle\\n')\n",
    "for row in contact_interp:\n",
    "    ostr='{},{},{},{},{}\\n'.format(row[0],row[1],row[2],row[3],row[4])\n",
    "    f.write(ostr)\n",
    "f.close()\n",
    "f=open(tmp_path+'interpolated_combined.csv','w')\n",
    "f.write('X,Y,l,m,n,dip,dip_dir\\n')\n",
    "for row in combo_interp:\n",
    "    ostr='{},{},{},{},{},{},{}\\n'.format(row[0],row[1],row[2],row[3],row[4],row[5],row[6])\n",
    "    f.write(ostr)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:56.860703Z",
     "start_time": "2020-06-02T06:36:56.191699Z"
    }
   },
   "outputs": [],
   "source": [
    "if(spacing<0):\n",
    "    spacing=-(bbox[2]-bbox[0])/spacing\n",
    "x=int((bbox[2]-bbox[0])/spacing)+1\n",
    "y=int((bbox[3]-bbox[1])/spacing)+1\n",
    "print(x,y)\n",
    "dip_grid=np.ones((y,x))\n",
    "dip_grid=dip_grid*-999\n",
    "dip_dir_grid=np.ones((y,x))\n",
    "dip_dir_grid=dip_dir_grid*-999\n",
    "contact_grid=np.ones((y,x))\n",
    "contact_grid=dip_dir_grid*-999\n",
    "for row in combo_interp:\n",
    "    r=int((row[1]-bbox[1])/spacing)\n",
    "    c=int((row[0]-bbox[0])/spacing)\n",
    "    dip_grid[r,c]=float(row[5])\n",
    "    dip_dir_grid[r,c]=float(row[6])\n",
    "\n",
    "for row in contact_interp:\n",
    "    r=int((row[1]-bbox[1])/spacing)\n",
    "    c=int((row[0]-bbox[0])/spacing)\n",
    "    contact_grid[r,c]=float(row[4])\n",
    "\n",
    "print('interpolated dips')\n",
    "plt.imshow(dip_grid, cmap=\"hsv\",origin='lower',vmin=-90,vmax=90)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:57.360703Z",
     "start_time": "2020-06-02T06:36:56.869698Z"
    }
   },
   "outputs": [],
   "source": [
    "print('interpolated dip directions')\n",
    "       \n",
    "plt.imshow(dip_dir_grid, cmap=\"hsv\",origin='lower',vmin=0,vmax=360)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:57.848448Z",
     "start_time": "2020-06-02T06:36:57.367700Z"
    }
   },
   "outputs": [],
   "source": [
    "print('interpolated contacts')\n",
    "       \n",
    "plt.imshow(contact_grid, cmap=\"hsv\",origin='lower',vmin=-360,vmax=360)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process fault geometry\n",
    "Save Faults as decimated points and representative orientation  \n",
    "Then, for each  fault string:\n",
    "- incementally advance along polyline every at each inter-node (no point in doing more?)\n",
    "- find local stratigraphy 10m to left and right of fault\n",
    "  \n",
    "Once full fault has been traversed:\n",
    "- Find list of contacts left \n",
    "- Find equivalent contacts on right\n",
    "- use interpolated orientations to estimate minimum true offset assuming vertical displacement and store \n",
    "- if no equivalent found, flag as domain fault and find min strat offset for contact, use cumulative minimum thickness estimate and store with flag (not implemented)\n",
    "- estimate median & sd of minimum fault offset and store with flag (not implemented)\n",
    "\n",
    "Local Orientations\n",
    "Since much of the code is the same, we benefit by calculating local orientation data either side of fault so that geomodeller/gempy have satisfied fault compartment orientation data## Save fault as contact info and and orientation info make vertical (for the moment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:36:58.310449Z",
     "start_time": "2020-06-02T06:36:57.855445Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "m2l_geometry.save_faults(tmp_path+'faults_clip.shp',output_path,dtm,dtb,dtb_null,workflow['cover_map'],c_l,fault_decimate,min_fault_length,fault_dip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:37:52.854783Z",
     "start_time": "2020-06-02T06:36:58.316449Z"
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "faults=pd.read_csv(output_path+'faults.csv')\n",
    "faults_len=len(faults)\n",
    "\n",
    "if(faults_len>0):\n",
    "    m2l_interpolation.process_fault_throw_and_near_faults_from_grid(tmp_path,output_path,dtm_reproj_file,dtb,dtb_null,workflow['cover_map'],c_l,dst_crs,bbox,\n",
    "                                                                scheme,dip_grid,dip_dir_grid,x,y,spacing)\n",
    "\n",
    "    m2l_utils.plot_points(output_path+'fault_displacements3.csv',geol_clip, 'apparent_displacement','X','Y',False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process plutons\n",
    "\n",
    "For each instruve but not sill polygon, find older neighbours and store decimated contact points. Also store dipping contact orientations (user defined, just because) with four possible sub-surface configurations:\n",
    "\n",
    "<b>saucers: \\\\_+++_/ <br>\n",
    "batholiths: +++/__ __ _\\\\+++  <br> \n",
    "domes: /+++\\\\ <br>\n",
    "pendants: +++\\\\_  _/+++ <br>\n",
    "</b>\n",
    "  \n",
    "Saves out orientations and contact points, as well as updated group level stratigraphic column.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:37:52.975329Z",
     "start_time": "2020-06-02T06:37:52.875786Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "bbox=(minx,miny,maxx,maxy)\n",
    "\n",
    "pluton_dip=str(pluton_dip)\n",
    "\n",
    "dist_buffer=10\n",
    "\n",
    "m2l_geometry.process_plutons(tmp_path,output_path,geol_clip,local_paths,dtm,dtb,dtb_null,workflow['cover_map'],pluton_form,pluton_dip,contact_decimate,c_l)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract faults and basal contacts of groups from seismic section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:37:53.024318Z",
     "start_time": "2020-06-02T06:37:52.984324Z"
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(workflow['seismic_section']):\n",
    "    seismic_line_file=data_path+'seismic_line_10GA-CP1_rev.shp'   #input geology file (if local)\n",
    "    seismic_line = gpd.read_file(seismic_line_file) #import map\n",
    "    seismic_line.plot(figsize=(10,10),edgecolor='#000000',linewidth=0.2) #display map\n",
    "    display(seismic_line)\n",
    "\n",
    "\n",
    "    seismic_bbox_file=data_path+'seismic_bbox.shp'   #input geology file (if local)\n",
    "    seismic_bbox = gpd.read_file(seismic_bbox_file) #import map\n",
    "    seismic_bbox.set_index('POSITION',inplace=True)\n",
    "\n",
    "    seismic_interp_file=data_path+'seismic_interp.shp'   #input geology file (if local)\n",
    "    seismic_interp = gpd.read_file(seismic_interp_file) #import map\n",
    "    seismic_interp.plot(column='FEATURE',figsize=(10,10),edgecolor='#000000',linewidth=0.5) #display map\n",
    "    display(seismic_interp)\n",
    "\n",
    "    surface_cut=2000\n",
    "\n",
    "    m2l_geometry.extract_section(tmp_path,output_path,seismic_line,seismic_bbox,seismic_interp,dtm,dtb,dtb_null,workflow['cover_map'],surface_cut)\n",
    "\n",
    "    contacts=pd.read_csv(output_path+'contacts4.csv',\",\")\n",
    "    seismic_contacts=pd.read_csv(output_path+'seismic_base.csv',\",\")\n",
    "    all_contacts=pd.concat([contacts,seismic_contacts],sort=False)\n",
    "    all_contacts.to_csv (output_path+'contacts4.csv', index = None, header=True)\n",
    "\n",
    "    faults=pd.read_csv(output_path+'faults.csv',\",\")\n",
    "    seismic_faults=pd.read_csv(output_path+'seismic_faults.csv',\",\")\n",
    "    all_faults=pd.concat([faults,seismic_faults],sort=False)\n",
    "    all_faults.to_csv (output_path+'faults.csv', index = None, header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propagate dips along contacts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:37:57.153856Z",
     "start_time": "2020-06-02T06:37:53.039322Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "if(workflow['contact_dips']):\n",
    "    orientations=pd.read_csv(output_path+'orientations.csv',\",\")\n",
    "    contact_dip=-999\n",
    "    contact_orientation_decimate=5\n",
    "    m2l_geometry.save_basal_contacts_orientations_csv(contacts,orientations,geol_clip,tmp_path,output_path,dtm,dtb,\n",
    "                            dtb_null,workflow['cover_map'],contact_orientation_decimate,c_l,contact_dip,dip_grid,spacing,bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate formation thickness and normalised formation thickness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:38:05.469082Z",
     "start_time": "2020-06-02T06:37:57.161852Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "if(workflow['formation_thickness']):\n",
    "\n",
    "    geology_file=tmp_path+'basal_contacts.shp'\n",
    "    contact_decimate=5\n",
    "    null_scheme='null'\n",
    "    m2l_interpolation.save_contact_vectors(geology_file,tmp_path,dtm,dtb,dtb_null,workflow['cover_map'],bbox,c_l,null_scheme,contact_decimate)\n",
    "    \n",
    "    buffer =5000\n",
    "    max_thickness_allowed=10000\n",
    "\n",
    "    m2l_geometry.calc_thickness_with_grid(tmp_path,output_path,buffer,max_thickness_allowed,\n",
    "                                          c_l,bbox,dip_grid,dip_dir_grid,x,y,spacing)\n",
    "    m2l_geometry.normalise_thickness(output_path)\n",
    "    \n",
    "    m2l_utils.plot_points(output_path+'formation_thicknesses_norm.csv',geol_clip,'norm_th','x','y',False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creates fold axial trace points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:38:05.498083Z",
     "start_time": "2020-06-02T06:38:05.479082Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "if(workflow['fold_axial_traces']):\n",
    "\n",
    "    m2l_geometry.save_fold_axial_traces(tmp_path+'folds_clip.shp',output_path,dtm,dtb,dtb_null,workflow['cover_map'],c_l,fold_decimate)\n",
    "\n",
    "    #Save fold axial trace near-hinge orientations\n",
    "    fat_step=750         # how much to step out normal to fold axial trace\n",
    "    close_dip=-999       #dip to assign to all new orientations (-999= use local interpolated dip)\n",
    "\n",
    "    m2l_geometry.save_fold_axial_traces_orientations(tmp_path+'folds_clip.shp',output_path,tmp_path,dtm,dtb,dtb_null,workflow['cover_map'],c_l,dst_crs,\n",
    "                                                     fold_decimate,fat_step,close_dip,scheme,bbox,spacing,dip_grid,dip_dir_grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data to ensure it meets modelling requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:38:08.027612Z",
     "start_time": "2020-06-02T06:38:05.533086Z"
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m2l_geometry.tidy_data(output_path,tmp_path,use_gcode3,use_interpolations,use_fat,pluton_form,inputs,workflow)\n",
    "dtm.close()\n",
    "if(workflow['cover_map']):\n",
    "    dtb.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate polarity of original bedding orientation data (not used yet in final calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:38:08.045611Z",
     "start_time": "2020-06-02T06:38:08.033612Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "if(workflow['polarity']):\n",
    "    m2l_geometry.save_orientations_with_polarity(output_path+'orientations.csv',output_path,c_l,tmp_path+'basal_contacts.shp',tmp_path+'all_sorts.csv',)\n",
    "\n",
    "    m2l_utils.plot_points(output_path+'orientations_polarity.csv',geol_clip,'polarity','X','Y',True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate minimum fault offset from stratigraphy and stratigraphic fault offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:38:14.593318Z",
     "start_time": "2020-06-02T06:38:08.052612Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "if(workflow['strat_offset']):\n",
    "    m2l_geometry.fault_strat_offset(output_path,c_l,dst_crs,output_path+'formation_summary_thicknesses.csv', tmp_path+'all_sorts.csv',tmp_path+'faults_clip.shp',tmp_path+'geol_clip.shp',output_path+'fault_dimensions.csv')\n",
    "\n",
    "\n",
    "    m2l_utils.plot_points(output_path+'fault_strat_offset3.csv',geol_clip,'min_offset','X','Y',True)\n",
    "    m2l_utils.plot_points(output_path+'fault_strat_offset3.csv',geol_clip,'strat_offset','X','Y',True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse fault-fault topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:38:15.206377Z",
     "start_time": "2020-06-02T06:38:14.598316Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "m2l_topology.parse_fault_relationships(graph_path,tmp_path,output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# loop2gemodeller test \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:38:31.275540Z",
     "start_time": "2020-06-02T06:38:15.211380Z"
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if(workflow['model_engine']=='geomodeller'):\n",
    "    from datetime import datetime\n",
    "    import shutil\n",
    "\n",
    "    \n",
    "    m2l_topology.check_near_fault_contacts(tmp_path+'faults_clip.shp',tmp_path+'all_sorts_clean.csv',\n",
    "                                           output_path+'fault_dimensions.csv',output_path+'group-fault-relationships.csv',\n",
    "                                           output_path+'contacts_clean.csv',c_l,dst_crs)\n",
    "\n",
    "    nowtime=datetime.now().isoformat(timespec='minutes')   \n",
    "    model_name=test_data_name+'_'+nowtime.replace(\":\",\"-\").replace(\"T\",\"-\")\n",
    "    os.mkdir(test_data_path+'/'+model_name)\n",
    "    save_faults=True\n",
    "    compute_etc=True\n",
    "    t1 = time.time()\n",
    "    m2l_export.loop2geomodeller(model_name,test_data_path,tmp_path,output_path,'../dtm/dtm_rp.tif',bbox,\n",
    "                                model_top,model_base,save_faults,compute_etc,workflow)\n",
    "    t2 = time.time()\n",
    "    os.chdir(test_data_path+'/'+model_name)\n",
    "    %system geomodellerbatch.exe -batch m2l.taskfile\n",
    "    t3 = time.time()\n",
    "    #%system geomodellerbatch.exe -batch m2l_compute.taskfile\n",
    "    t4 = time.time()\n",
    "    print(\"m2l\",(t1-t0)/60.0,\"export process\",(t2-t1)/60.0,\"batch process\",(t3-t2)/60.0,\"batch calculate\",(t4-t3)/60.0,\"minutes\")\n",
    "    #shutil.copy('../tmp','.')\n",
    "    #shutil.copy('../output','.')\n",
    "    #shutil.copy('../graph','.')\n",
    "    #shutil.copy('../dtm','.')\n",
    "    os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loopstructural test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:38:31.297815Z",
     "start_time": "2020-06-02T06:38:31.281543Z"
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if(workflow['model_engine']=='loopstructural'):\n",
    "    f=open(tmp_path+'bbox.csv','w')\n",
    "    f.write('minx,miny,maxx,maxy,lower,upper\\n')\n",
    "    ostr='{},{},{},{},{},{}\\n'.format(minx,miny,maxx,maxy,model_base,model_top)\n",
    "    f.write(ostr)\n",
    "    f.close()\n",
    "    t1 = time.time()\n",
    "    print(\"m2l\",(t1-t0)/60.0,\"minutes\")    \n",
    "    import lavavu\n",
    "    from pyamg import solve\n",
    "\n",
    "    m2l_export.loop2LoopStructural(output_path+'formation_thicknesses.csv',output_path+'orientations.csv',output_path+'contacts4.csv',tmp_path,bbox)\n",
    "    t2 = time.time()\n",
    "    print(\"m2l\",(t1-t0)/60.0,\"LoopStructural\",(t2-t1)/60.0,\"Total\",(t2-t0)/60.0,\"minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gempy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:38:31.327540Z",
     "start_time": "2020-06-02T06:38:31.303541Z"
    },
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if(workflow['model_engine']=='gempy'):\n",
    "\n",
    "    t1 = time.time()\n",
    "    import importlib\n",
    "    importlib.reload(m2l_export)\n",
    "\n",
    "\n",
    "    vtk=False\n",
    "    m2l_export.loop2gempy(test_data_name,tmp_path,vtk_path,output_path+'orientations_clean.csv',\n",
    "                                    output_path+'contacts_clean.csv',tmp_path+'groups_clean.csv',\n",
    "                                    bbox,model_base, model_top,vtk,dtm_reproj_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# noddy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:38:31.378548Z",
     "start_time": "2020-06-02T06:38:31.335540Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "if(workflow['model_engine']=='noddy'):\n",
    "\n",
    "    import pynoddy.history\n",
    "    import networkx as nx\n",
    "    #Read a csv file with the vertices of the faults\n",
    "    #see notes in the bottom of the notebook for instructions on how to generate such vertices files\n",
    "    t1 = time.time()\n",
    "    \n",
    "    scale=1.5 #  scales mdoel to fit predefined volume (complete hack)\n",
    "    \n",
    "    # load fault coordinates\n",
    "    faultsxy=pd.read_csv(output_path+'faults.csv')\n",
    "\n",
    "    #load fault graph, remove cyclic loops and find (non-unique) age-ordered list \n",
    "    G=nx.read_gml(tmp_path+\"fault_network.gml\")\n",
    "    cycles=list(nx.simple_cycles(G))\n",
    "    for c in cycles:\n",
    "        G.remove_edge(c[0], c[1])\n",
    "    faults=nx.topological_sort(G)\n",
    "\n",
    "    # write out Noe format format file\n",
    "    file=open(tmp_path+'faults_for_noe.csv','w')\n",
    "    file.write('id,DipDirecti,X,Y\\n')\n",
    "    for f in faults:\n",
    "            fxy=faultsxy[faultsxy[\"formation\"]==f.replace(\"\\n\",\"\")]\n",
    "            #display(f.replace(\"\\n\",\"\"))\n",
    "            for ind,xy in fxy.iterrows():\n",
    "                ostr=f.replace('\\n','')+',West,'+str(xy['X']/scale)+','+str(xy['Y']/scale)+'\\n'\n",
    "                file.write(ostr)\n",
    "    file.close()\n",
    "    \n",
    "    csvfile = tmp_path+'faults_for_noe.csv'\n",
    "    CsvFaultData = pd.read_csv(csvfile)\n",
    "\n",
    "    #how much does the fault slip relative to the fault length\n",
    "    SlipParam = 0.1\n",
    "\n",
    "    #the xyz origin of the model you will be generating\n",
    "    xy_origin=[minx/scale,miny/scale, 1200-4000]\n",
    "\n",
    "    #Get information about each parameter in Noddy format\n",
    "    #The output from the function is a dictionary with lists of the fault parameters\n",
    "    noddyFormattedFaultData =  pynoddy.history.setUpFaultRepresentation(CsvFaultData,\n",
    "                                                        xy_origin=xy_origin, \n",
    "                                                        SlipParam=SlipParam)\n",
    "\n",
    "    #Create a dictionary with the stratigraphy information\n",
    "    StratDict = {}\n",
    "    StratDict['Heights'] = [2000, 2500, 3000, 3700]\n",
    "    StratDict['Names'] = ['Intrusive', 'Felsic', 'Mafic','Sed'] \n",
    "    StratDict['Density'] =  [2.65, 2.5, 2.4, 2.3] \n",
    "    StratDict['MagSus'] = [0.0015, 0.0012, 0.0018, 0.001]\n",
    "\n",
    "    #Now make the history file\n",
    "    filename = output_path+'faultmodel.his'\n",
    "    noddyFormattedFaultData =  pynoddy.history.createPyNoddyHistoryFile(noddyFormattedFaultData, StratDict, filename=filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-02T06:38:31.412547Z",
     "start_time": "2020-06-02T06:38:31.390541Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "if(workflow['model_engine']=='noddy'):\n",
    "    import vtkplotter as vtkP\n",
    "    import itkwidgets\n",
    "    import k3d\n",
    "    import pynoddy.output\n",
    "    import pynoddy.history\n",
    "\n",
    "    modelfile = output_path+'faultmodel.his'\n",
    "\n",
    "    # Determine the path to the noddy executable\n",
    "    noddy_path = '../../pynoddy-new/noddyapp/noddy_win64.exe'\n",
    "\n",
    "    # Where you would like to place all your output files\n",
    "    outputfolder = tmp_path\n",
    "\n",
    "    # choose what software to use for visualizing the model\n",
    "    #you can also choose to change to itkwidgets, k3d, False (popup), or panel\n",
    "    #you might need to install packages depending on what you choose\n",
    "    vtkP.settings.embedWindow('k3d') \n",
    "\n",
    "    # create a plot in vtkplotter\n",
    "    plot = vtkP.Plotter(axes=1, bg='white', interactive=1)\n",
    "\n",
    "    # call the plotting function\n",
    "    points = pynoddy.output.CalculatePlotStructure(modelfile, plot, noddy_path, \n",
    "                                           outputfolder=outputfolder,\n",
    "                                           LithologyOpacity=0.2, outputOption=0)\n",
    "    plot.show(viewup='z')\n",
    "    t2 = time.time()\n",
    "\n",
    "    print(\"m2l\",(t1-t0)/60.0,\"noddy\",(t2-t1)/60.0,\"Total\",(t2-t0)/60.0,\"minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
